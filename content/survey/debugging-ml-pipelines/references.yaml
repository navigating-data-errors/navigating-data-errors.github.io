- bibTeX: "@article{pedregosa2011scikit,
    \ title={Scikit-learn: Machine learning in Python},
    \ author={Fabian Pedregosa and Gael Varoquaux and Alexandre Gramfort and Vincent Michel and Bertrand Thirion and Olivier Grisel and Mathieu Blondel and Peter Prettenhofer and Ron Weiss and Vincent Dubourg and Jake Vanderplas and Alexandre Passos and David Cournapeau and Matthieu Brucher and Matthieu Perrot and Edouard Duchesnay},
    \ journal={the Journal of machine Learning research},
    \ volume={12},
    \ pages={2825--2830},
    \ year={2011},
    \ publisher={JMLR. org}
    \ }"
  title: 'Scikit-learn: Machine Learning in Python'
  authors:
  - F Pedregosa
  - G Varoquaux
  - A Gramfort
  - V Michel
  - B Thirion
  - O Grisel
  - M Blondel
  - P Prettenhofer
  - R Weiss
  - V Dubourg
  - J Vanderplas
  - A Passos
  - D Cournapeau
  - M Brucher
  - M Perrot
  - E Duchesnay
  abstract: |
    Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for 
    medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to 
    non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, 
    documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD 
    license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation 
    can be downloaded from http://scikit-learn.sourceforge.net.
  year: 2011
  entryType: article
  firstPage: 1
  links:
    paper: https://www.jmlr.org/papers/v12/pedregosa11a.html
    website: https://scikit-learn.org/
    code: https://www.github.com/scikit-learn/scikit-learn
  id: pedregosa2011scikit
  thumbnail: /images/papers/pedregosa2011scikit.png
  type: publication
  venueLong: Journal of Machine Learning Research
  venueShort: JMLR
  venueTrack: null
  groups: Frameworks for Building ML Pipelines
  tag: ml-pipelines-general

- bibTeX: "@inproceedings{baylor2017tfx,
    \ author = {Baylor, Denis and Breck, Eric and Cheng, Heng-Tze and Fiedel, Noah and Foo, Chuan Yu and Haque, Zakaria and Haykal, Salem and Ispir, Mustafa and Jain, Vihan and Koc, Levent and Koo, Chiu Yuen and Lew, Lukasz and Mewald, Clemens and Modi, Akshay Naresh and Polyzotis, Neoklis and Ramesh, Sukriti and Roy, Sudip and Whang, Steven Euijong and Wicke, Martin and Wilkiewicz, Jarek and Zhang, Xin and Zinkevich, Martin},
    \ title = {TFX: A TensorFlow-Based Production-Scale Machine Learning Platform},
    \ year = {2017},
    \ isbn = {9781450348874},
    \ publisher = {Association for Computing Machinery},
    \ address = {New York, NY, USA},
    \ url = {https://doi.org/10.1145/3097983.3098021},
    \ doi = {10.1145/3097983.3098021},
    booktitle = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
    \ pages = {1387-1395},
    \ numpages = {9},
    \ keywords = {continuous training, end-to-end platform, large-scale machine learning},
    \ location = {Halifax, NS, Canada},
    \ series = {KDD '17}
    \ }"
  title: 'TFX: A TensorFlow-Based Production-Scale Machine Learning Platform'
  authors:
  - D Baylor
  - E Breck
  - H Cheng
  - N Fiedel
  - CY Foo
  - Z Haque
  - S Haykal
  - M Ispir
  - V Jain
  - L Koc
  - CY Koo
  - L Lew
  - C Mewald
  - A Modi
  - S Polyzotis
  - S Ramesh
  - S Roy
  - SE Whang
  - M Wicke
  - J Wilkiewicz
  - X Zhang
  - M Zinkevich
  abstract: |
    Creating and maintaining a platform for reliably producing and deploying machine learning models requires careful 
    orchestration of many components---a learner for generating models based on training data, modules for analyzing 
    and validating both data as well as models, and finally infrastructure for serving models in production. This 
    becomes particularly challenging when data changes over time and fresh models need to be produced continuously. 
    Unfortunately, such orchestration is often done ad hoc using glue code and custom scripts developed by individual 
    teams for specific use cases, leading to duplicated effort and fragile systems with high technical debt.

    We present TensorFlow Extended (TFX), a TensorFlow-based general-purpose machine learning platform implemented at 
    Google. By integrating the aforementioned components into one platform, we were able to standardize the components, 
    simplify the platform configuration, and reduce the time to production from the order of months to weeks, while 
    providing platform stability that minimizes disruptions.

    We present the case study of one deployment of TFX in the Google Play app store, where the machine learning models 
    are refreshed continuously as new data arrive. Deploying TFX led to reduced custom code, faster experiment cycles, 
    and a 2% increase in app installs resulting from improved data and model analysis.
  year: 2017
  entryType: inproceedings
  firstPage: 1
  links:
    paper: https://dl.acm.org/doi/abs/10.1145/3097983.3098021
    website: https://tensorflow.github.io/tfx/
    code: https://github.com/tensorflow/tfx
  id: baylor2017tfx
  thumbnail: /images/papers/baylor2017tfx.png
  type: publication
  venueLong: Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining
  venueShort: KDD
  venueTrack: null
  groups: Frameworks for Building ML Pipelines
  tag: ml-pipelines-general

- bibTeX: "@article{meng2016mllib,
    \ title={Mllib: Machine learning in apache spark},
    \ author={Xiangrui Meng and Joseph Bradley and Burak Yavuz and Evan Sparks and Shivaram Venkataraman and Davies Liu and Jeremy Freeman and DB Tsai and Manish Amde and Sean Owen and Doris Xin and Reynold Xin and Michael J. Franklin and Reza Zadeh and Matei Zaharia and Ameet Talwalkar},
    \ journal={Journal of Machine Learning Research},
    \ volume={17},
    \ number={34},
    \ pages={1--7},
    \ year={2016}
    \ }"
  title: 'MLlib: Machine Learning in Apache Spark'
  authors:
  - X Meng
  - J Bradley
  - B Yavuz
  - E Sparks
  - S Venkataraman
  - D Liu
  - J Freeman
  - DB Tsai
  - M Amde
  - S Owen
  - D Xin
  - R Xin
  - MJ Franklin
  - R Zadeh
  - M Zaharia
  - A Talwalkar
  abstract: |
    Apache Spark is a popular open-source platform for large-scale data processing that is well-suited for iterative 
    machine learning tasks. In this paper we present MLlib, Spark's open- source distributed machine learning library. 
    MLlib provides efficient functionality for a wide range of learning settings and includes several underlying 
    statistical, optimization, and linear algebra primitives. Shipped with Spark, MLlib supports several languages 
    and provides a high-level API that leverages Spark's rich ecosystem to simplify the development of end-to-end 
    machine learning pipelines. MLlib has experienced a rapid growth due to its vibrant open-source community of 
    over 140 contributors, and includes extensive documentation to support further growth and to let users quickly 
    get up to speed.
  year: 2016
  entryType: article
  firstPage: 1
  links:
    paper: https://www.jmlr.org/papers/v17/15-237.html
    website: https://spark.apache.org/mllib/
    code: https://github.com/apache/spark/tree/master/mllib
  id: meng2016mllib
  thumbnail: /images/papers/meng2016mllib.png
  type: publication
  venueLong: Journal of Machine Learning Research
  venueShort: JMLR
  venueTrack: null
  groups: Frameworks for Building ML Pipelines
  tag: ml-pipelines-general

- bibTeX: "@article{boehm2019systemds,
    \ title={SystemDS: A declarative machine learning system for the end-to-end data science lifecycle},
    \ author={Matthias Boehm and Iulian Antonov and Sebastian Baunsgaard and Mark Dokter and Robert Ginthoer and Kevin Innerebner and Florijan Klezin and Stefanie Lindstaedt and Arnab Phani and Benjamin Rath and Berthold Reinwald and Shafaq Siddiqi and Sebastian Benjamin Wrede},
    \ journal={10th Annual Conference on Innovative Data Systems Research},
    \ year={2020}
    \ }"
  title: 'SystemDS: A Declarative Machine Learning System for the End-to-End Data Science Lifecycle'
  authors:
  - M Boehm
  - I Antonov
  - S Baunsgaard
  - M Dokter
  - R Ginthoer
  - K Innerebner
  - F Klezin
  - S Lindstaedt
  - A Phani
  - B Rath
  - B Reinwald
  - S Siddiqi
  - SB Wrede
  abstract: |
    Machine learning (ML) applications become increasingly common in many domains. ML systems to execute these 
    workloads include numerical computing frameworks and libraries, ML algorithm libraries, and specialized systems 
    for deep neural networks and distributed ML. These systems focus primarily on efficient model training and 
    scoring. However, the data science process is exploratory, and deals with underspecified objectives and a wide 
    variety of heterogeneous data sources. Therefore, additional tools are employed for data engineering and 
    debugging, which requires boundary crossing, unnecessary manual effort, and lacks optimization across the 
    lifecycle. In this paper, we introduce SystemDS, an open source ML system for the end-to-end data science 
    lifecycle from data integration, cleaning, and preparation, over local, distributed, and federated ML model 
    training, to debugging and serving. To this end, we aim to provide a stack of declarative language abstractions 
    for the different lifecycle tasks, and users with different expertise. We describe the overall system 
    architecture, explain major design decisions (motivated by lessons learned from Apache SystemML), and 
    discuss key features and research directions. Finally, we provide preliminary results that show the 
    potential of end-to-end lifecycle optimization.
  year: 2020
  entryType: article
  firstPage: 1
  links:
    paper: https://phaniarnab.github.io/assets/papers/cidr2020.pdf
    website: https://systemds.apache.org/
    code: https://github.com/apache/systemds
  id: boehm2019systemds
  thumbnail: /images/papers/boehm2019systemds.png
  type: publication
  venueLong: 10th Annual Conference on Innovative Data Systems Research
  venueShort: CIDR
  venueTrack: null
  groups: Frameworks for Building ML Pipelines
  tag: ml-pipelines-general

- bibTeX: "@inproceedings{xin2021production,
    \ title={Production machine learning pipelines: Empirical analysis and optimization opportunities},
    \ author={Xin, Doris and Miao, Hui and Parameswaran, Aditya and Polyzotis, Neoklis},
    \ booktitle={Proceedings of the 2021 international conference on management of data},
    \ pages={2639--2652},
    \ year={2021}
    \ }"
  title: 'Production Machine Learning Pipelines: Empirical Analysis and Optimization Opportunities'
  authors:
  - D Xin
  - H Miao
  - A Parameswaran
  - N Polyzotis
  abstract: |
    Machine learning (ML) is now commonplace, powering data-driven applications in various organizations. Unlike 
    the traditional perception of ML in research, ML production pipelines are complex, with many interlocking 
    analytical components beyond training, whose sub-parts are often run multiple times on overlapping subsets 
    of data. However, there is a lack of quantitative evidence regarding the lifespan, architecture, frequency, 
    and complexity of these pipelines to understand how data management research can be used to make them more 
    efficient, effective, robust, and reproducible. To that end, we analyze the provenance graphs of 3000 
    production ML pipelines at Google, comprising over 450,000 models trained, spanning a period of over four 
    months, in an effort to understand the complexity and challenges underlying production ML. Our analysis 
    reveals the characteristics, components, and topologies of typical industry-strength ML pipelines at various 
    granularities. Along the way, we introduce a specialized data model for representing and reasoning about 
    repeatedly run components in these ML pipelines, which we call model graphlets. We identify several rich 
    opportunities for optimization, leveraging traditional data management ideas. We show how targeting even 
    one of these opportunities, i.e., identifying and pruning wasted computation that does not translate to model 
    deployment, can reduce wasted computation cost by 50% without compromising the model deployment cadence.
  year: 2021
  entryType: inproceedings
  firstPage: 1
  links:
    paper: https://dl.acm.org/doi/abs/10.1145/3448016.3457566
  id: xin2021production
  thumbnail: /images/papers/xin2021production.png
  type: publication
  venueLong: Proceedings of the 2021 International Conference on Management of Data
  venueShort: SIGMOD
  venueTrack: null
  groups: Studies of ML Pipelines
  tag: ml-pipelines-studies

- bibTeX: "@article{psallidas2022data,
    \ title={Data science through the looking glass: Analysis of millions of github notebooks and ml. net pipelines},
    \ author={Psallidas, Fotis and Zhu, Yiwen and Karlaš, Bojan and Henkel, Jordan and Interlandi, Matteo and Krishnan, Subru and Kroth, Brian and Emani, Venkatesh and Wu, Wentao and Zhang, Ce and Weimer, Markus and Floratou, Avrilia and Curino, Carlo and Karanasos, Konstantinos},
    \ journal={ACM SIGMOD Record},
    \ volume={51},
    \ number={2},
    \ pages={30--37},
    \ year={2022},
    \ publisher={ACM New York, NY, USA}
    \ }"
  title: 'Data Science Through the Looking Glass: Analysis of Millions of GitHub Notebooks and ML.NET Pipelines'
  authors:
  - F Psallidas
  - Y Zhu
  - B Karlaš
  - J Henkel
  - M Interlandi
  - S Krishnan
  - B Kroth
  - V Emani
  - W Wu
  - C Zhang
  - M Weimer
  - A Floratou
  - C Curino
  - K Karanasos
  abstract: |
    The recent success of machine learning (ML) has led to an explosive growth of systems and applications built by 
    an ever-growing community of system builders and data science (DS) practitioners. This quickly shifting panorama, 
    however, is challenging for system builders and practitioners alike to follow. In this paper, we set out to 
    capture this panorama through a wide-angle lens, performing the largest analysis of DS projects to date, focusing 
    on questions that can advance our understanding of the field and determine investments. Specifically, we 
    download and analyze (a) over 8M notebooks publicly available on GITHUB and (b) over 2M enterprise ML pipelines 
    developed within Microsoft. Our analysis includes coarse-grained statistical characterizations, finegrained 
    analysis of libraries and pipelines, and comparative studies across datasets and time. We report a large number 
    of measurements for our readers to interpret and draw actionable conclusions on (a) what system builders should 
    focus on to better serve practitioners and (b) what technologies should practitioners rely on.
  year: 2022
  entryType: article
  firstPage: 1
  links:
    paper: https://sigmodrecord.org/publications/sigmodRecord/2206/pdfs/05_Research_Psallidas.pdf
  id: psallidas2022data
  thumbnail: /images/papers/psallidas2022data.png
  type: publication
  venueLong: ACM SIGMOD Record
  venueShort: SIGMOD Record
  venueTrack: null
  groups: Studies of ML Pipelines
  tag: ml-pipelines-studies

- bibTeX: "@inproceedings{green2007provenance,
    \ title={Provenance semirings},
    \ author={Green, Todd J and Karvounarakis, Grigoris and Tannen, Val},
    \ booktitle={Proceedings of the twenty-sixth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems},
    \ pages={31--40},
    \ year={2007}
    \ }"
  title: 'Provenance Semirings'
  authors:
  - T Green
  - G Karvounarakis
  - V Tannen
  abstract: |
    We show that relational algebra calculations for incomplete databases, probabilistic databases, bag semantics 
    and why-provenance are particular cases of the same general algorithms involving semirings. This further 
    suggests a comprehensive provenance representation that uses semirings of polynomials. We extend these 
    considerations to datalog and semirings of formal power series. We give algorithms for datalog provenance 
    calculation as well as datalog evaluation for incomplete and probabilistic databases. Finally, we show that 
    for some semirings containment of conjunctive queries is the same as for standard set semantics.
  year: 2007
  entryType: inproceedings
  firstPage: 1
  links:
    paper: https://courses.cs.washington.edu/courses/cse544/11wi/lectures/tannen-semirings.pdf
  id: green2007provenance
  thumbnail: /images/papers/green2007provenance.png
  type: publication
  venueLong: Proceedings of the Twenty-Sixth ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems
  venueShort: PODS
  venueTrack: null
  group: Fundamental Concepts
  tag: ml-pipelines-fundamentals

- bibTeX: "@inproceedings{karlas2023data,
    \ title={Data Debugging with Shapley Importance over Machine Learning Pipelines},
    \ author={Karlaš}, Bojan and Dao, David and Interlandi, Matteo and Schelter, Sebastian and Wu, Wentao and Zhang, Ce},
    \ booktitle={The Twelfth International Conference on Learning Representations}
    \ year={2024}
    \ }"
  title: 'Data Debugging with Shapley Importance over Machine Learning Pipelines'
  authors:
  - B Karlaš
  - D Dao
  - M Interlandi
  - S Schelter
  - W Wu
  - C Zhang
  abstract: |
    When a machine learning (ML) model exhibits poor quality (e.g., poor accuracy or fairness), the problem can 
    often be traced back to errors in the training data. Being able to discover the data examples that are the most 
    likely culprits is a fundamental concern that has received a lot of attention recently. One prominent way to 
    measure "data importance" with respect to model quality is the Shapley value. Unfortunately, existing methods 
    only focus on the ML model in isolation, without considering the broader ML pipeline for data preparation and 
    feature extraction, which appears in the majority of real-world ML code. This presents a major limitation to 
    applying existing methods in practical settings. In this paper, we propose Datascope, a method for efficiently 
    computing Shapley-based data importance over ML pipelines. We introduce several approximations that lead to 
    dramatic improvements in terms of computational speed. Finally, our experimental evaluation demonstrates that 
    our methods are capable of data error discovery that is as effective as existing Monte Carlo baselines, and in 
    some cases even outperform them. We release our code as an open-source data debugging library.
  year: 2024
  entryType: inproceedings
  firstPage: 1
  links:
    paper: https://openreview.net/forum?id=qxGXjWxabq
    website: https://ease.ml/datascope
    code: https://www.github.com/easeml/datascope
  id: karlas2023data
  thumbnail: /images/papers/karlas2023data.png
  type: publication
  venueLong: The Twelfth International Conference on Learning Representations
  venueShort: ICLR
  venueTrack: null
  group: Debugging ML Pipelines
  tag: ml-pipelines-debugging

- bibTeX: "@inproceedings{wu2020complaint,
    \ title={Complaint-driven training data debugging for query 2.0},
    \ author={Wu, Weiyuan and Flokas, Lampros and Wu, Eugene and Wang, Jiannan},
    \ booktitle={Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
    \ pages={1317--1334},
    \ year={2020}
    \ }"
  title: 'Complaint-Driven Training Data Debugging for Query 2.0'
  authors:
  - W Wu
  - L Flokas
  - E Wu
  - J Wang
  abstract: |
    As the need for machine learning (ML) increases rapidly across all industry sectors, there is a significant 
    interest among commercial database providers to support "Query 2.0", which integrates model inference into SQL 
    queries. Debugging Query 2.0 is very challenging since an unexpected query result may be caused by the bugs in 
    training data (e.g., wrong labels, corrupted features). In response, we propose Rain, a complaint-driven training 
    data debugging system. Rain allows users to specify complaints over the query's intermediate or final output, 
    and aims to return a minimum set of training examples so that if they were removed, the complaints would be 
    resolved. To the best of our knowledge, we are the first to study this problem. A naive solution requires 
    retraining an exponential number of ML models. We propose two novel heuristic approaches based on influence 
    functions which both require linear retraining steps. We provide an in-depth analytical and empirical analysis 
    of the two approaches and conduct extensive experiments to evaluate their effectiveness using four real-world 
    datasets. Results show that Rain achieves the highest recall@k among all the baselines while still returns 
    results interactively.
  year: 2020
  entryType: inproceedings
  firstPage: 1
  links:
    paper: https://www2.cs.sfu.ca/~jnwang/papers/sigmod2020-rain-full-version.pdf
  id: wu2020complaint
  thumbnail: /images/papers/wu2020complaint.png
  type: publication
  venueLong: Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data
  venueShort: SIGMOD
  venueTrack: null
  group: Debugging ML Pipelines
  tag: ml-pipelines-debugging

- bibTeX: "@inproceedings{flokas2022complaint,
    \ title={Complaint-driven training data debugging at interactive speeds},
    \ author={Flokas, Lampros and Wu, Weiyuan and Liu, Yejia and Wang, Jiannan and Verma, Nakul and Wu, Eugene},
    \ booktitle={Proceedings of the 2022 International Conference on Management of Data},
    \ pages={369--383},
    \ year={2022}
    \ }"
  title: 'Complaint-Driven Training Data Debugging at Interactive Speeds'
  authors:
  - L Flokas
  - W Wu
  - Y Liu
  - J Wang
  - N Verma
  - E Wu
  abstract: |
    Modern databases support queries that perform model inference (inference queries). Although powerful and 
    widely used, inference queries are susceptible to incorrect results if the model is biased due to training 
    data errors. Recently, prior work Rain proposed complaint-driven data debugging which uses user-specified 
    errors in the output of inference queries (Complaints) to rank erroneous training examples that most likely 
    caused the complaint. This can help users better interpret results and debug training sets. Rain combined 
    influence analysis from the ML literature with relaxed query provenance polynomials from the DB literature to 
    approximate the derivative of complaints w.r.t. training examples. Although effective, the runtime is O(|T|d), 
    where T and d are the training set and model sizes, due to its reliance on the model's second order derivatives 
    (the Hessian). On a Wide Resnet Network (WRN) model with 1.5 million parameters, it takes >1 minute to debug a 
    complaint. We observe that most complaint debugging costs are independent of the complaint, and that modern 
    models are overparameterized. In response, Rain++ uses precomputation techniques, based on non-trivial insights 
    unique to data debugging, to reduce debugging latencies to a constant factor independent of model size. We also 
    develop optimizations when the queried database is known apriori, and for standing queries over streaming 
    databases. Combining these optimizations in Rain++ ensures interactive debugging latencies (~1ms) on models 
    with millions of parameters.
  year: 2022
  entryType: inproceedings
  firstPage: 1
  links:
    paper: https://dl.acm.org/doi/pdf/10.1145/3514221.3517849
  id: flokas2022complaint
  thumbnail: /images/papers/flokas2022complaint.png
  type: publication
  venueLong: Proceedings of the 2022 International Conference on Management of Data
  venueShort: SIGMOD
  venueTrack: null
  group: Debugging ML Pipelines
  tag: ml-pipelines-debugging

- bibTeX: "@article{grafberger2022data,
    \ title={Data distribution debugging in machine learning pipelines},
    \ author={Grafberger, Stefan and Groth, Paul and Stoyanovich, Julia and Schelter, Sebastian},
    \ journal={The VLDB Journal},
    \ volume={31},
    \ number={5},
    \ pages={1103--1126},
    \ year={2022},
    \ publisher={Springer}
    \ }"
  title: 'Data Distribution Debugging in Machine Learning Pipelines'
  authors:
  - S Grafberger
  - P Groth
  - J Stoyanovich
  - S Schelter
  abstract: |
    Machine learning (ML) is increasingly used to automate impactful decisions, and the risks arising from this 
    widespread use are garnering attention from policy makers, scientists, and the media. ML applications are 
    often brittle with respect to their input data, which leads to concerns about their correctness, reliability, 
    and fairness. In this paper, we describe mlinspect, a library that helps diagnose and mitigate technical bias 
    that may arise during preprocessing steps in an ML pipeline. We refer to these problems collectively as data 
    distribution bugs. The key idea is to extract a directed acyclic graph representation of the dataflow from a 
    preprocessing pipeline and to use this representation to automatically instrument the code with predefined 
    inspections. These inspections are based on a lightweight annotation propagation approach to propagate metadata 
    such as lineage information from operator to operator. In contrast to existing work, mlinspect operates on 
    declarative abstractions of popular data science libraries like estimator/transformer pipelines and does not 
    require manual code instrumentation. We discuss the design and implementation of the mlinspect library and 
    give a comprehensive end-to-end example that illustrates its functionality.
  year: 2022
  entryType: article
  firstPage: 1
  links:
    paper: https://stefan-grafberger.com/mlinspect-journal.pdf
    code: https://github.com/stefan-grafberger/mlinspect
  id: grafberger2022data
  thumbnail: /images/papers/grafberger2022data.png
  type: publication
  venueLong: The VLDB Journal
  venueShort: VLDBJ
  venueTrack: null
  group: Debugging ML Pipelines
  tag: ml-pipelines-debugging

- bibTeX: "@inproceedings{schelter2023proactively,
    \ title={Proactively screening machine learning pipelines with arguseyes},
    \ author={Schelter, Sebastian and Grafberger, Stefan and Guha, Shubha and Karlas, Bojan and Zhang, Ce},
    \ booktitle={Companion of the 2023 International Conference on Management of Data},
    \ pages={91--94},
    \ year={2023}
    \ }"
  title: "Proactively Screening Machine Learning Pipelines with ArgusEyes"
  authors:
  - S Schelter
  - S Grafberger
  - S Guha
  - B Karlaš
  - C Zhang
  abstract: |
    Software systems that learn from data with machine learning (ML) are ubiquitous. ML pipelines in these 
    applications often suffer from a variety of data-related issues, such as data leakage, label errors or fairness 
    violations, which require reasoning about complex dependencies between their inputs and outputs. These issues are 
    usually only detected in hindsight after deployment, after they caused harm in production. We demonstrate 
    ArgusEyes, a system which enables data scientists to proactively screen their ML pipelines for data-related 
    issues as part of continuous integration. ArgusEyes instruments, executes and screens ML pipelines for 
    declaratively specified pipeline issues, and analyzes data artifacts and their provenance to catch potential 
    problems early before deployment to production. We demonstrate our system for three scenarios: detecting 
    mislabeled images in a computer vision pipeline, spotting data leakage in a price prediction pipeline, and 
    addressing fairness violations in a credit scoring pipeline.
  year: 2023
  entryType: inproceedings
  firstPage: 1
  links:
    paper: https://dl.acm.org/doi/abs/10.1145/3555041.3589682
  id: schelter2023proactively
  thumbnail: /images/papers/schelter2023proactively.png
  type: publication
  venueLong: Companion of the 2023 International Conference on Management of Data (Demo)
  venueShort: SIGMOD
  venueTrack: Demo
  group: Debugging ML Pipelines
  tag: ml-pipelines-debugging
